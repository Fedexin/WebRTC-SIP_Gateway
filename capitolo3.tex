\chapter{Architettura di Integrazione}
\label{cap:architettura}

Dopo aver analizzato le caratteristiche teoriche di SIP e WebRTC nel capitolo precedente, questo capitolo descrive l'implementazione pratica del gateway realizzato per abilitare l'interoperabilità tra i due protocolli. L'architettura proposta affronta le sfide tecniche identificate, dalla trasformazione dei messaggi di segnalazione alla conversione dei flussi media, fino alla gestione del NAT traversal. Verranno presentati i componenti principali del sistema, le tecnologie adottate e le scelte implementative, accompagnate da estratti significativi del codice sviluppato.

\section{Architettura Generale del Sistema}
\label{sec:architettura-generale}

L'architettura implementata si basa su un modello a quattro livelli, ciascuno con responsabilità ben definite. La Figura \ref{fig:arch_diagram} mostra la disposizione dei componenti e i protocolli di comunicazione utilizzati.

\begin{figure}[h!]
\centering
\begin{tikzpicture}[
  node distance=1.5cm,
  component/.style={rectangle, draw, fill=blue!10, text width=3.5cm, text centered, rounded corners, minimum height=1.2cm, drop shadow},
  arrow/.style={->, >=stealth, thick},
  protocol/.style={font=\small\ttfamily, color=gray}
]

% WebRTC Client
\node[component] (webclient) {Client WebRTC\\(Browser)};

% Signaling Server
\node[component, below=of webclient] (server) {Server di Segnalazione\\(Node.js)};

% SIP Gateway Plugin
\node[component, below=of server] (sipgw) {SIP Gateway Plugin};

% RTPEngine
\node[component, left=2cm of sipgw] (rtpengine) {RTPEngine\\(Media Relay)};

% SIP Server
\node[component, below=of sipgw] (sipserver) {SIP Server\\(mjSIP)};

% SIP Client
\node[component, below=of sipserver] (sipclient) {Client SIP\\(mjSIP UA)};

% Arrows
\draw[arrow] (webclient) -- node[right, protocol] {WebSocket} (server);
\draw[arrow] (server) -- (sipgw);
\draw[arrow] (sipgw) -- node[right, protocol] {SIP/UDP} (sipserver);
\draw[arrow] (sipserver) -- node[right, protocol] {SIP/UDP} (sipclient);
\draw[arrow] (sipgw) -- node[above, protocol] {NG Protocol} (rtpengine);
\draw[arrow, dashed] (webclient.west) -- ++(-1.5,0) |- node[left, protocol, pos=0.25] {SRTP/DTLS} (rtpengine.north);
\draw[arrow, dashed] (rtpengine.south) |- node[left, protocol, pos=0.75] {RTP/UDP} ++(-1.5,-1.5) -- (sipclient.west);

\end{tikzpicture}
\caption{Architettura generale del gateway WebRTC-SIP: i componenti principali e i protocolli di comunicazione utilizzati.}
\label{fig:arch_diagram}
\end{figure}

\subsection{Componenti Principali}

Il sistema è composto da quattro componenti fondamentali:

\begin{itemize}
\item \textbf{Client WebRTC}: Un'applicazione web sviluppata in React e TypeScript che utilizza le API WebRTC native del browser. Si occupa della cattura dei dispositivi multimediali (\texttt{getUserMedia}), della gestione delle connessioni peer-to-peer (\texttt{RTCPeerConnection}) e dello scambio di messaggi di segnalazione tramite WebSocket.

\item \textbf{Server di Segnalazione}: Un server Node.js basato sul protocollo WebSocket che funge da punto centrale per il routing dei messaggi. Gestisce la registrazione degli utenti, l'inoltro delle offerte e risposte SDP e coordina le chiamate tra client WebRTC e utenti SIP.

\item \textbf{SIP Gateway Plugin}: Il cuore del sistema di integrazione. Implementa un parser SIP completo conforme alla RFC 3261, gestisce le transazioni e i dialoghi SIP, e si occupa della traduzione bidirezionale tra messaggi WebSocket e messaggi SIP.

\item \textbf{RTPEngine}: Un media relay che opera a livello di trasporto RTP/SRTP. Si occupa della transcodifica tra il traffico SRTP cifrato (utilizzato da WebRTC) e il traffico RTP in chiaro (utilizzato dai sistemi SIP tradizionali), oltre a gestire la conversione tra codec audio differenti.
\end{itemize}

\subsection{Flusso di una Chiamata WebRTC verso SIP}

Il processo di stabilimento di una chiamata da un client WebRTC verso un terminale SIP attraversa diverse fasi, che coinvolgono tutti i componenti dell'architettura:

\begin{enumerate}
\item Il client WebRTC invia una richiesta di chiamata al server di segnalazione tramite WebSocket, specificando l'URI SIP del destinatario (ad esempio \texttt{sip:bob@192.168.1.209}).

\item Il server inoltra la richiesta al SIP Gateway Plugin, che genera un messaggio SIP \texttt{INVITE} valido, includendo l'SDP offerta dal client WebRTC.

\item Prima di inviare l'\texttt{INVITE}, il gateway interagisce con RTPEngine attraverso il comando \texttt{offer()}, passando l'SDP WebRTC. RTPEngine alloca risorse per la sessione media e restituisce un SDP tradotto compatibile con SIP (rimuovendo ICE candidates, fingerprint DTLS e altri attributi WebRTC-specific).

\item L'\texttt{INVITE} viene trasmesso al server SIP via UDP sulla porta 5060. Il server SIP, che ha precedentemente ricevuto una \texttt{REGISTER} dal client SIP, conosce la sua posizione e inoltra la richiesta.

\item Il client SIP risponde con \texttt{180 Ringing} e successivamente \texttt{200 OK}, includendo il proprio SDP di risposta.

\item Il gateway riceve il \texttt{200 OK}, estrae l'SDP e lo passa nuovamente a RTPEngine con il comando \texttt{answer()}. RTPEngine configura il relay media e restituisce un SDP arricchito con ICE candidates e attributi DTLS.

\item L'SDP arricchito viene inviato al client WebRTC come risposta alla chiamata. Il client imposta la descrizione remota e la connessione media viene stabilita.

\item Il gateway completa la transazione SIP inviando un \texttt{ACK} al client SIP, confermando la ricezione del \texttt{200 OK}.

\item A questo punto, i flussi RTP bidirezionali vengono instradati attraverso RTPEngine: SRTP cifrato tra browser e RTPEngine, RTP in chiaro tra RTPEngine e client SIP.
\end{enumerate}

Questo flusso evidenzia il ruolo centrale di RTPEngine come intermediario necessario per colmare il gap tra la sicurezza obbligatoria di WebRTC e i sistemi SIP legacy che operano senza cifratura.

\section{Tecnologie e Strumenti Utilizzati}
\label{sec:tecnologie}

La scelta delle tecnologie è stata guidata dalla necessità di conformità agli standard, dalla facilità di integrazione e dalla disponibilità di strumenti di debugging robusti.

\subsection{Backend: Server di Segnalazione}

Il server di segnalazione è stato realizzato in \textbf{Node.js} (versione 14 o superiore) per sfruttare il modello event-driven e l'ecosistema npm. Le librerie principali utilizzate sono:

\begin{itemize}
\item \textbf{ws} (v8.18.0): Implementazione WebSocket conforme alla RFC 6455, utilizzata per la comunicazione full-duplex con i client browser.
\item \textbf{winston} (v3.18.3): Sistema di logging strutturato con supporto per multiple \textit{transport} (console, file, remote logging). Essenziale per il debugging di sistemi distribuiti.
\item \textbf{rtpengine-client} (v0.4.12): Client per comunicare con RTPEngine attraverso il protocollo NG (basato su bencode), che consente di inviare comandi \texttt{offer}, \texttt{answer} e \texttt{delete}.
\item \textbf{dotenv} (v17.2.3): Gestione delle variabili d'ambiente, utilizzato per configurare indirizzi IP, porte e parametri operativi.
\end{itemize}

Il server espone inoltre un endpoint HTTP sulla porta 8080 per health check, facilitando il monitoraggio in ambienti di produzione.

\subsection{Frontend: Client WebRTC}

Il client è un'applicazione \textit{Single Page Application} (SPA) sviluppata con le seguenti tecnologie:

\begin{itemize}
\item \textbf{React} (v19.1.1): Framework dichiarativo per la costruzione di interfacce utente reattive.
\item \textbf{TypeScript} (v5.8.3): Superset tipizzato di JavaScript, che garantisce type safety e migliora la manutenibilità del codice.
\item \textbf{Vite} (v7.1.6): Build tool moderno basato su ESBuild, che offre tempi di ricompilazione estremamente rapidi durante lo sviluppo.
\item \textbf{Tailwind CSS} (v4.1.17): Framework CSS utility-first per la stilizzazione rapida e consistente.
\item \textbf{Radix UI}: Libreria di componenti accessibili utilizzata per dialog, dropdown e altri elementi dell'interfaccia.
\item \textbf{TanStack Query} (v5.89.0): Gestione dello stato server-side con caching e sincronizzazione automatica.
\item \textbf{Sonner} (v2.0.7): Sistema di notifiche toast leggero e performante.
\end{itemize}

L'applicazione è strutturata seguendo il pattern \textit{container/presentational components}, con una netta separazione tra logica di business (contenuta nei servizi e negli hook React) e componenti di presentazione.

\subsection{Stack SIP: mjSIP}

Per la parte SIP è stato scelto \textbf{mjSIP} (v2.0.5), uno stack SIP scritto interamente in Java, conforme alla RFC 3261. mjSIP è stato preferito ad alternative come PJSIP o Asterisk per le seguenti ragioni:

\begin{itemize}
\item \textbf{Semplicità}: Poche dipendenze esterne, configurazione tramite file testuale.
\item \textbf{Portabilità}: Scritto in Java, eseguibile su qualsiasi piattaforma con JVM.
\item \textbf{Modularità}: Separazione chiara tra stack SIP di base, User Agent e server proxy/registrar.
\item \textbf{Didattico}: Codice ben documentato, utile per comprendere i meccanismi interni di SIP.
\end{itemize}

mjSIP è stato configurato per operare su porta UDP 5060, con supporto per \texttt{rport} e \texttt{symmetric\_rtp} per il NAT traversal.

\subsection{Media Processing: RTPEngine}

\textbf{RTPEngine} è un proxy RTP ad alte prestazioni sviluppato originariamente per il progetto Sipwise. È stato deployato tramite Docker utilizzando l'immagine \texttt{fonoster/rtpengine}.

Le funzionalità chiave utilizzate sono:

\begin{itemize}
\item \textbf{Cifratura/Decifratura}: Gestione trasparente di SRTP con scambio chiavi DTLS per WebRTC e RTP in chiaro per SIP.
\item \textbf{ICE Processing}: Generazione e gestione di ICE candidates per i client WebRTC.
\item \textbf{Codec Transcoding}: Conversione tra Opus (utilizzato da WebRTC) e G.711 PCMU/PCMA (utilizzati da SIP).
\item \textbf{NAT Traversal}: Rilevamento automatico degli endpoint dietro NAT e instradamento del traffico RTP verso gli indirizzi corretti.
\end{itemize}

RTPEngine viene controllato attraverso il protocollo NG su UDP porta 22222. Il protocollo utilizza bencode (lo stesso formato del protocollo BitTorrent) per serializzare comandi e risposte.

\subsection{Testing e Debugging}

Per verificare il corretto funzionamento del sistema sono stati utilizzati diversi strumenti:

\begin{itemize}
\item \textbf{Script di Test Automatizzati}: Due script Node.js (\texttt{test/rtpengine-test.js} e \texttt{test/server-test.js}) che verificano rispettivamente la corretta interazione con RTPEngine e la gestione delle transazioni SIP base.

\item \textbf{Wireshark}: Utilizzato per l'analisi del traffico di rete, con filtri specifici per SIP (\texttt{sip}), RTP (\texttt{rtp}), STUN (\texttt{stun}) e il protocollo NG di RTPEngine (\texttt{udp.port == 22222}).

\item \textbf{chrome://webrtc-internals/}: Strumento integrato in Chrome/Chromium per il debug WebRTC. Permette di visualizzare in tempo reale lo stato delle \texttt{RTCPeerConnection}, le negoziazioni SDP, i candidati ICE raccolti e le statistiche RTP (packet loss, jitter, bitrate).

\item \textbf{Docker Logs}: Monitoraggio continuo dei log di RTPEngine tramite \texttt{docker logs -f rtpengine-sip-webrtc}, utile per verificare le operazioni di \texttt{offer}, \texttt{answer} e \texttt{delete}.

\item \textbf{Winston Logging}: Tutti i componenti Node.js utilizzano Winston per generare log strutturati in formato JSON, salvati in \texttt{logs/combined.log} e \texttt{logs/error.log}.
\end{itemize}

Un tipico flusso di test prevede:

\begin{enumerate}
\item Avvio di RTPEngine: \texttt{cd server/rtpengine \&\& docker-compose up -d}
\item Avvio del server di segnalazione: \texttt{cd server \&\& npm start}
\item Avvio del SIP server: \texttt{cd sip-server \&\& ./run-server.sh}
\item Avvio del client SIP: \texttt{cd sip-client \&\& ./run-client1.sh}
\item Apertura del client WebRTC: \texttt{cd WebRTC-client \&\& npm run dev}
\item Cattura del traffico con Wireshark
\item Esecuzione di chiamate bidirezionali e verifica dei log
\end{enumerate}

\section{Implementazione}
\label{sec:implementazione}

Questa sezione descrive i dettagli implementativi dei componenti principali, corredata da estratti di codice significativi.

\subsection{Server di Segnalazione WebSocket}

Il server di segnalazione rappresenta il punto di ingresso per i client WebRTC. Il file \texttt{signaling-server.js} implementa un server WebSocket che gestisce la registrazione degli utenti e l'instradamento dei messaggi.

\begin{codelisting}[listing options={language=ES6}]{Inizializzazione del Server WebSocket}
const WebSocket = require('ws');
const http = require('http');
const winston = require('winston');

const logger = winston.createLogger({
  level: 'debug',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({ filename: 'logs/combined.log' })
  ]
});

const server = http.createServer((req, res) => {
  if (req.url === '/health') {
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ status: 'ok', uptime: process.uptime() }));
  }
});

const wss = new WebSocket.Server({ server });
const users = new Map(); // username -> { ws, lastSeen }

wss.on('connection', (ws) => {
  logger.info('New WebSocket connection');

  ws.on('message', (data) => {
    try {
      const message = JSON.parse(data);
      handleMessage(ws, message);
    } catch (error) {
      logger.error('Failed to parse message', { error: error.message });
    }
  });
});

server.listen(8080, () => {
  logger.info('Signaling server started on port 8080');
});
\end{codelisting}

La gestione della registrazione degli utenti è semplice ma efficace: ogni client invia un messaggio \texttt{register} con il proprio username, e il server mantiene una mappa che associa username a connessione WebSocket.

\begin{codelisting}[listing options={language=ES6}]{Registrazione Utenti}
function handleMessage(ws, message) {
  switch (message.type) {
    case 'register':
      users.set(message.username, { ws, lastSeen: Date.now() });
      logger.info('User registered', { username: message.username });
      ws.send(JSON.stringify({ type: 'registered' }));
      break;

    case 'offer':
    case 'answer':
    case 'ice-candidate':
      // Inoltro del messaggio al destinatario
      const target = users.get(message.targetUser);
      if (target && target.ws.readyState === WebSocket.OPEN) {
        target.ws.send(JSON.stringify(message));
      }
      break;

    case 'call-request':
      // Se il destinatario è un URI SIP, delega al SIP Gateway
      if (message.to.startsWith('sip:')) {
        handleSipCall(ws, message);
      }
      break;
  }
}
\end{codelisting}

\subsection{SIP Gateway Plugin}

Il componente più complesso dell'architettura è il SIP Gateway Plugin (\texttt{plugins/sipGateway.js}), che implementa un parser SIP completo, la gestione delle transazioni e l'integrazione con RTPEngine.

\subsubsection{Parser SIP}

Il parser SIP gestisce la struttura dei messaggi secondo la RFC 3261. Un messaggio SIP è composto da una linea iniziale (request o status line), header e un body opzionale (tipicamente SDP).

\begin{codelisting}[listing options={language=ES6}]{Parser SIP - Request Line e Headers}
parseSipMessage(data) {
  const lines = data.toString().split('\r\n');
  let i = 0;

  // Parse request line (es: INVITE sip:bob@192.168.1.209 SIP/2.0)
  const requestLine = lines[i++];
  const [method, uri, version] = requestLine.split(' ');

  const headers = {};

  // Parse headers fino alla riga vuota
  while (i < lines.length && lines[i] !== '') {
    let line = lines[i];

    // Gestione header multi-linea (RFC 3261: iniziano con spazio)
    while (i + 1 < lines.length &&
           (lines[i + 1].startsWith(' ') || lines[i + 1].startsWith('\t'))) {
      line += ' ' + lines[++i].trim();
    }

    const colonIndex = line.indexOf(':');
    if (colonIndex > 0) {
      const name = line.substring(0, colonIndex).trim();
      const value = line.substring(colonIndex + 1).trim();
      headers[name.toLowerCase()] = value;
    }
    i++;
  }

  // Il body inizia dopo la riga vuota
  const body = lines.slice(i + 1).join('\r\n');

  return { method, uri, version, headers, body };
}
\end{codelisting}

\subsubsection{Gestione Transazioni INVITE}

La gestione delle transazioni INVITE richiede particolare attenzione per evitare risposte duplicate dovute alle ritrasmissioni UDP. Viene utilizzata una mappa di transazioni indicizzata per call-id, CSeq e branch parameter del Via header.

\begin{codelisting}[listing options={language=ES6}]{Gestione INVITE e Ritrasmissioni}
async handleInvite(request, rinfo) {
  const callId = request.headers['call-id'];
  const cseq = request.headers['cseq'];
  const via = this.parseVia(request.headers['via']);
  const branch = via.params.branch;

  const transactionKey = `${callId}-${cseq}-${branch}`;

  // Rilevamento ritrasmissioni
  if (this.inviteTransactions.has(transactionKey)) {
    this.logger.debug('INVITE retransmission detected', { callId });
    const tx = this.inviteTransactions.get(transactionKey);

    // Re-invia l'ultima risposta
    if (tx.lastResponse) {
      this.sendResponse(request, tx.lastResponse.status,
                        tx.lastResponse.reason, rinfo);
    }
    return;
  }

  // Nuova transazione
  this.inviteTransactions.set(transactionKey, {
    callId,
    state: 'proceeding',
    lastResponse: null
  });

  // Invia 100 Trying
  this.sendResponse(request, 100, 'Trying', rinfo);

  // Prosegui con la logica della chiamata...
}
\end{codelisting}

\subsubsection{Integrazione con RTPEngine}

L'integrazione con RTPEngine avviene attraverso il protocollo NG. Per una chiamata entrante da SIP verso WebRTC, il gateway effettua una chiamata \texttt{offer()} passando l'SDP SIP e specificando che il risultato deve essere compatibile con WebRTC (SRTP/DTLS, ICE).

\begin{codelisting}[listing options={language=ES6}]{RTPEngine Offer per SDP WebRTC}
async offerToRtpEngine(sipSdp, callId, fromTag) {
  const offerPayload = {
    'call-id': callId,
    'from-tag': fromTag,
    'sdp': sipSdp,
    'transport-protocol': 'UDP/TLS/RTP/SAVPF', // WebRTC
    'ICE': 'force',                             // Genera ICE candidates
    'DTLS': 'passive',                          // Server DTLS
    'rtcp-mux': ['offer'],
    'codec': {
      'strip': ['telephone-event'],             // Rimuovi DTMF
      'offer': ['opus', 'PCMU', 'PCMA']         // Codec supportati
    }
  };

  try {
    const result = await this.rtpengine.offer(
      this.config.rtpenginePort,
      this.config.rtpengineHost,
      offerPayload
    );

    this.logger.info('RTPEngine offer successful', {
      callId,
      sdpLength: result.sdp.length
    });

    return result.sdp; // SDP arricchito con ICE e DTLS
  } catch (error) {
    this.logger.error('RTPEngine offer failed', { error: error.message });
    throw error;
  }
}
\end{codelisting}

Quando il client WebRTC risponde con il proprio SDP, viene effettuata una chiamata \texttt{answer()} con la trasformazione inversa: da WebRTC verso SIP.

\begin{codelisting}[listing options={language=ES6}]{RTPEngine Answer per SDP SIP}
async answerToRtpEngine(webrtcSdp, callId, fromTag, toTag) {
  const answerPayload = {
    'call-id': callId,
    'from-tag': fromTag,
    'to-tag': toTag,
    'sdp': webrtcSdp,
    'transport-protocol': 'RTP/AVP',  // SIP tradizionale
    'ICE': 'remove',                  // Rimuovi ICE
    'rtcp-mux': ['demux'],
    'codec': {
      'strip': ['opus'],              // SIP legacy non supporta Opus
      'offer': ['PCMU', 'PCMA']
    }
  };

  const result = await this.rtpengine.answer(
    this.config.rtpenginePort,
    this.config.rtpengineHost,
    answerPayload
  );

  return result.sdp; // SDP compatibile con SIP
}
\end{codelisting}

\subsection{Client WebRTC Service}

Il client WebRTC è strutturato attorno a un servizio (\texttt{WebRTCService.ts}) che incapsula tutta la logica di gestione delle \texttt{RTCPeerConnection}. Il servizio implementa il \textit{Perfect Negotiation Pattern}, raccomandato dalla specifica WebRTC per gestire le collisioni durante la negoziazione.

\subsubsection{Perfect Negotiation Pattern}

Il Perfect Negotiation Pattern assegna a ciascun peer un ruolo (\textit{polite} o \textit{impolite}) basato su un criterio deterministico. In caso di collisione (entrambi i peer inviano contemporaneamente un'offerta), il peer \textit{polite} effettua un rollback della propria offerta e accetta quella del peer \textit{impolite}.

\begin{codelisting}[listing options={language=ES6}]{Perfect Negotiation - Gestione Collisioni}
async handleSignalingMessage(message: SignalingMessage) {
  // Determinazione ruolo: lessicograficamente minore è polite
  this.polite = this.username < message.from;

  const offerCollision =
    (message.type === 'offer') &&
    (this.makingOffer ||
     this.peerConnection.signalingState !== 'stable');

  this.ignoreOffer = !this.polite && offerCollision;

  if (this.ignoreOffer) {
    console.log('Impolite peer ignoring colliding offer');
    return;
  }

  // Polite peer effettua rollback
  if (this.polite && offerCollision) {
    await this.peerConnection.setLocalDescription({ type: 'rollback' });
  }

  await this.peerConnection.setRemoteDescription(
    new RTCSessionDescription({
      type: message.type,
      sdp: message.sdp
    })
  );

  // Se è un'offer, genera automaticamente answer
  if (message.type === 'offer') {
    const answer = await this.peerConnection.createAnswer();
    await this.peerConnection.setLocalDescription(answer);
    this.sendMessage({ type: 'answer', sdp: answer.sdp });
  }
}
\end{codelisting}

\subsubsection{Normalizzazione SDP}

Un problema riscontrato durante lo sviluppo riguarda la generazione dell'SDP da parte di alcuni browser (in particolare Firefox con \texttt{mozilla...THIS\_IS\_SDPARTA} come username nell'origin line). Questo username contiene caratteri che violano la RFC 4566 e viene rifiutato da mjSIP. È stata implementata una funzione di normalizzazione che corregge queste anomalie.

\begin{codelisting}[listing options={language=ES6}]{Normalizzazione SDP per Compatibilità}
private normalizeSdp(sdp: string): string {
  const lines = sdp.split('\r\n');

  for (let i = 0; i < lines.length; i++) {
    // Fix origin line (o=username sessionId version netType addrType address)
    if (lines[i].startsWith('o=')) {
      const parts = lines[i].split(' ');
      const username = parts[0].substring(2);

      // Fix username problematici
      if (username.includes('...') || username.includes('mozilla')) {
        parts[0] = `o=${this.username || 'webrtc'}`;
        lines[i] = parts.join(' ');
      }

      // Fix IP 0.0.0.0
      if (parts[5] === '0.0.0.0') {
        parts[5] = this.gatewayIP;
        lines[i] = parts.join(' ');
      }
    }

    // Fix connection line
    if (lines[i].startsWith('c=') && lines[i].includes('0.0.0.0')) {
      lines[i] = `c=IN IP4 ${this.gatewayIP}`;
    }
  }

  return lines.join('\r\n');
}
\end{codelisting}

\subsection{Configurazioni}

Il sistema utilizza diverse configurazioni per adattarsi agli ambienti di deployment. Le configurazioni principali sono gestite tramite file \texttt{.env} e file di configurazione specifici per mjSIP.

\subsubsection{Configurazione Server Gateway}

\begin{codelisting}[listing options={language=bash}]{Variabili d'Ambiente del Server}
# Server
PORT=8080
ENABLE_SIP_GATEWAY=true
LOG_LEVEL=debug

# SIP
SIP_SERVER_HOST=192.168.1.209
SIP_SERVER_PORT=5060
LOCAL_SIP_PORT=5060

# RTPEngine
RTPENGINE_HOST=127.0.0.1
RTPENGINE_PORT=22222

# Network
PUBLIC_IP=auto
\end{codelisting}

Il parametro \texttt{PUBLIC\_IP=auto} consente al server di rilevare automaticamente l'indirizzo IP pubblico tramite query STUN, utile in ambienti NAT.

\subsubsection{Configurazione RTPEngine}

RTPEngine è deployato tramite Docker Compose con la seguente configurazione:

\begin{codelisting}[listing options={language=ES6}]{Docker Compose per RTPEngine}
version: '3.8'
services:
  rtpengine:
    image: fonoster/rtpengine:latest
    container_name: rtpengine-sip-webrtc
    network_mode: "host"
    environment:
      RTPENGINE_PUBLIC_IP: 192.168.1.127
      RTPENGINE_PORT_MIN: 10000
      RTPENGINE_PORT_MAX: 20000
      RTPENGINE_BIND_NG_PORT: 22222
      RTPENGINE_BIND_HTTP_PORT: 9090
    restart: unless-stopped
\end{codelisting}

L'uso di \texttt{network\_mode: "host"} consente a RTPEngine di utilizzare direttamente lo stack di rete dell'host, necessario per la corretta gestione di ICE e NAT traversal.

\subsubsection{Configurazione Client SIP (mjsip)}

\begin{codelisting}[listing options={language=bash}]{Configurazione mjsip User Agent}
via_addr=192.168.1.209
host_port=5062
sip_user=client1
proxy=192.168.1.209
registrar=sip:192.168.1.209:5060
from_url=sip:client1@192.168.1.209

# Media configuration
media=audio 50000 RTP/AVP { 0 PCMU 8000 160, 8 PCMA 8000 160 }

# NAT traversal
use_rport=yes
symmetric_rtp=yes
\end{codelisting}

Il parametro \texttt{use\_rport=yes} abilita il supporto per RFC 3581, che consente al proxy SIP di rispondere all'indirizzo e porta da cui ha effettivamente ricevuto la richiesta, risolvendo problemi di NAT simmetrico.

\section{Problematiche Riscontrate}
\label{sec:problematiche}

Durante lo sviluppo e il testing del gateway sono emerse diverse problematiche tecniche, ciascuna delle quali ha richiesto analisi approfondite e soluzioni specifiche.

\subsection{Compatibilità SDP tra WebRTC e SIP Legacy}

\textbf{Problema}: Il client SIP mjsip rifiutava sistematicamente l'SDP generato dal client WebRTC con l'errore \textit{"Invalid SDP origin line"}.

\textbf{Analisi}: L'origine del problema risiedeva in tre aspetti:

\begin{enumerate}
\item \textbf{Username non conforme}: Firefox genera SDP con \texttt{o=mozilla...THIS\_IS\_SDPARTA}, contenente caratteri non validi secondo RFC 4566.
\item \textbf{Indirizzo IP invalido}: In alcune configurazioni, l'SDP conteneva \texttt{0.0.0.0} come indirizzo di connessione.
\item \textbf{Attributi WebRTC-specific}: L'SDP WebRTC contiene numerosi attributi (\texttt{a=ice-ufrag}, \texttt{a=fingerprint}, \texttt{a=candidate}, ecc.) non riconosciuti da mjsip, che li considera errori sintattici.
\end{enumerate}

\textbf{Soluzione}: È stata implementata una procedura di pulizia dell'SDP in due fasi:

\begin{enumerate}
\item \textbf{Lato client}: Normalizzazione dell'origin line e delle connection line sostituendo username e IP problematici.
\item \textbf{Lato gateway}: Rimozione completa degli attributi WebRTC-specific prima di inoltrare l'SDP verso SIP. Il filtro rimuove linee che iniziano con \texttt{a=ice-}, \texttt{a=fingerprint}, \texttt{a=rtcp-mux}, \texttt{a=candidate}, \texttt{a=extmap}.
\end{enumerate}

Questa soluzione ha consentito la piena compatibilità con mjsip mantenendo al contempo la conformità agli standard WebRTC.

\subsection{Echo e Qualità Audio}

\textbf{Problema}: Durante le prime chiamate di test si verificava un forte eco e rumore di fondo, rendendo le conversazioni difficilmente comprensibili.

\textbf{Analisi}: L'analisi tramite \texttt{chrome://webrtc-internals/} ha rivelato che le funzionalità di elaborazione audio (echo cancellation, noise suppression, auto gain control) non erano abilitate nei constraints di \texttt{getUserMedia}.

\textbf{Soluzione}: Sono stati modificati i constraints audio come segue:

\begin{codelisting}[listing options={language=ES6}]{Constraints Audio Ottimizzati}
const constraints = {
  audio: {
    echoCancellation: true,
    noiseSuppression: true,
    autoGainControl: true,
    sampleRate: 48000
  },
  video: true
};

const stream = await navigator.mediaDevices.getUserMedia(constraints);
\end{codelisting}

Questa modifica ha risolto completamente il problema di eco e ha migliorato significativamente la qualità audio percepita. Il commit \texttt{c1a07c8} documenta questa correzione.

\subsection{ICE Candidates Mancanti}

\textbf{Problema}: In alcune sessioni, la connessione media falliva con l'errore \textit{"ICE connection failed"}, nonostante la segnalazione fosse completata correttamente.

\textbf{Analisi}: L'SDP veniva inviato al peer remoto prima che il processo di ICE gathering fosse completato. Questo comportava l'invio di un SDP privo di candidati ICE, rendendo impossibile la connessione P2P.

\textbf{Soluzione}: È stata implementata una funzione di attesa che monitora lo stato di \texttt{iceGatheringState}:

\begin{codelisting}[listing options={language=ES6}]{Attesa Completamento ICE Gathering}
private waitForIceGatheringComplete(timeout: number): Promise<void> {
  return new Promise((resolve, reject) => {
    if (this.peerConnection.iceGatheringState === 'complete') {
      resolve();
      return;
    }

    const timer = setTimeout(() => {
      this.peerConnection.removeEventListener(
        'icegatheringstatechange', handler
      );
      resolve(); // Timeout: procedi comunque con i candidati raccolti
    }, timeout);

    const handler = () => {
      if (this.peerConnection.iceGatheringState === 'complete') {
        clearTimeout(timer);
        this.peerConnection.removeEventListener(
          'icegatheringstatechange', handler
        );
        resolve();
      }
    };

    this.peerConnection.addEventListener(
      'icegatheringstatechange', handler
    );
  });
}
\end{codelisting}

Con questa modifica, l'SDP viene inviato solo dopo aver raccolto tutti i candidati ICE disponibili (o dopo un timeout di 3 secondi).

\subsection{Gestione Ritrasmissioni SIP}

\textbf{Problema}: Il gateway rispondeva multiple volte allo stesso \texttt{INVITE}, creando stati inconsistenti e chiamate duplicate.

\textbf{Analisi}: SIP utilizza UDP come trasporto, che non garantisce la consegna. Per compensare, i client SIP ritrasmettono le richieste seguendo i timer definiti in RFC 3261 (T1, T2). Il gateway non riconosceva le ritrasmissioni e le trattava come nuove chiamate.

\textbf{Soluzione}: È stata implementata una mappa di transazioni indicizzata per call-id, CSeq e branch parameter:

\begin{codelisting}[listing options={language=ES6}]{Rilevamento Ritrasmissioni}
const transactionKey = `${callId}-${cseq}-${branch}`;

if (this.inviteTransactions.has(transactionKey)) {
  const tx = this.inviteTransactions.get(transactionKey);
  this.sendResponse(request, tx.lastResponse.status,
                    tx.lastResponse.reason, rinfo);
  return; // Non processare nuovamente
}
\end{codelisting}

Questa soluzione garantisce che ogni ritrasmissione riceva esattamente la stessa risposta precedentemente inviata, mantenendo la conformità RFC 3261.

\subsection{NAT Traversal}

\textbf{Problema}: Le chiamate tra client su reti diverse fallivano, nonostante la segnalazione fosse corretta.

\textbf{Analisi}: I NAT modificano gli indirizzi IP e le porte dei pacchetti, ma i messaggi SIP contengono informazioni su IP e porte anche nel payload (header \texttt{Via}, \texttt{Contact}, SDP). Questo crea una discrepanza: il server riceve pacchetti da un indirizzo IP diverso da quello dichiarato nei messaggi.

\textbf{Soluzione}: Implementazione del supporto \texttt{rport} (RFC 3581):

\begin{codelisting}[listing options={language=ES6}]{Gestione NAT con rport}
handleNATForRequest(request, rinfo) {
  const via = this.parseVia(request.headers['via']);
  const viaHost = via.host;
  const viaPort = via.port || 5060;

  // Confronta indirizzo dichiarato vs indirizzo reale
  if (viaHost !== rinfo.address || viaPort !== rinfo.port) {
    // Client dietro NAT
    via.params.received = rinfo.address;
    via.params.rport = rinfo.port;

    this.logger.info('NAT detected', {
      declared: `${viaHost}:${viaPort}`,
      actual: `${rinfo.address}:${rinfo.port}`
    });
  }

  return via;
}
\end{codelisting}

Questa implementazione consente al client di scoprire il proprio indirizzo pubblico e porta mappata dal NAT, utilizzandoli per i successivi messaggi.

\section{Risultati Sperimentali}
\label{sec:risultati}

Durante lo sviluppo, il sistema è stato testato manualmente in ambiente LAN per verificarne il corretto funzionamento e identificare eventuali problematiche.

\bigskip
\noindent\textbf{Ambiente di Test}

I test sono stati condotti in ambiente LAN con la seguente configurazione:

\begin{itemize}
\item \textbf{Server Gateway}: Ubuntu 22.04, Node.js v18.16.0, 8GB RAM, Intel Core i7
\item \textbf{RTPEngine}: Docker container, 2 vCPU, 2GB RAM limit
\item \textbf{SIP Server/Client}: mjSIP 2.0.5, Java 11
\item \textbf{Client WebRTC}: Chrome 120, Firefox 121
\item \textbf{Rete}: LAN 1Gbps, latenza <1ms
\end{itemize}

\bigskip
\noindent\textbf{Osservazioni sulla Performance}

Durante i test manuali sono state effettuate chiamate bidirezionali tra client WebRTC e client SIP, monitorando il comportamento del sistema attraverso i log strutturati, Wireshark e lo strumento diagnostico \texttt{chrome://webrtc-internals/} di Chrome.

\medskip
\noindent\textit{Setup della Chiamata.}
Il tempo necessario per stabilire una chiamata (dal click sul pulsante "Call" fino all'inizio del flusso audio) si è rivelato dominato dalla fase di ICE gathering. Consultando \texttt{chrome://webrtc-internals/}, si è osservato che il browser impiega tipicamente 1-3 secondi per raccogliere i candidati ICE contattando server STUN esterni. Questo rappresenta il principale contributo alla latenza totale di setup, seguita dal DTLS handshake necessario per stabilire la connessione SRTP cifrata.

Le altre fasi (creazione offer/answer WebRTC, elaborazione RTPEngine, scambio messaggi SIP) sono risultate significativamente più rapide, completandosi nell'ordine delle decine di millisecondi.

\medskip
\noindent\textit{Qualità Audio.}
Una volta stabilita la chiamata, l'audio è risultato chiaro e privo di distorsioni evidenti in ambiente LAN. Monitorando le statistiche RTP tramite \texttt{chrome://webrtc-internals/}, si è osservato che:

\begin{itemize}
\item Il packet loss rimane trascurabile in condizioni di rete stabile (LAN)
\item Il jitter si mantiene a valori bassi, garantendo una conversazione fluida
\item Il bitrate audio riflette correttamente i codec negoziati: Opus per WebRTC (bitrate variabile), G.711 PCMU/PCMA per SIP (64 kbps fisso)
\end{itemize}

L'integrazione con RTPEngine per la transcodifica SRTP ↔ RTP non ha introdotto ritardi percepibili o degradazioni della qualità audio nelle condizioni di test.

\bigskip
\noindent\textbf{Limitazioni e Considerazioni}

Durante lo sviluppo sono emerse alcune limitazioni architetturali che meritano di essere evidenziate:

\begin{enumerate}
\item \textbf{Dipendenza da ICE gathering}: Il tempo di setup è fortemente influenzato dalla fase di raccolta candidati ICE, che dipende dalla disponibilità e latenza dei server STUN/TURN esterni. In scenari con connettività limitata o firewall restrittivi, questo tempo potrebbe aumentare significativamente.

\item \textbf{RTPEngine come single point of failure}: L'architettura attuale utilizza una singola istanza di RTPEngine per tutto il traffico media. In un deployment produttivo, questo rappresenterebbe un punto critico: il fallimento di RTPEngine causerebbe l'interruzione di tutte le chiamate attive. Sarebbe necessario implementare ridondanza con multiple istanze e load balancing.

\item \textbf{Scalabilità limitata}: RTPEngine è configurato con un range di porte RTP da 10000 a 20000, teoricamente supportando fino a 10000 sessioni simultanee. Tuttavia, il container Docker con 2 vCPU e 2GB RAM limiterebbe la scalabilità ben prima di raggiungere questo numero, richiedendo risorse hardware aggiuntive per scenari ad alto carico.

\item \textbf{Assenza di autenticazione}: Come discusso nelle scelte architetturali, il sistema non implementa meccanismi di autenticazione. Questo lo rende inadatto per deployment in ambienti non controllati o esposti a Internet senza modifiche sostanziali.

\item \textbf{Compatibilità limitata a mjSIP}: Le trasformazioni SDP implementate (normalizzazione username, case sensitivity dei protocolli, riposizionamento della linea c=) sono state specificamente progettate per mjSIP. Altri stack SIP potrebbero avere requisiti diversi o essere più tolleranti, richiedendo potenzialmente adattamenti.

\item \textbf{Codec transcoding}: Quando WebRTC negozia Opus e il client SIP supporta solo G.711, RTPEngine deve effettuare transcodifica in tempo reale. Questo processo è CPU-intensive e potrebbe diventare un bottleneck con molte chiamate concorrenti.
\end{enumerate}

\bigskip
\noindent\textbf{Considerazioni Finali}

I test manuali hanno dimostrato che l'architettura implementata è funzionale e capace di gestire l'interoperabilità tra WebRTC e SIP in ambiente controllato (LAN). Il sistema si stabilisce correttamente, l'audio è di buona qualità e i problemi tecnici identificati durante lo sviluppo (compatibilità SDP, gestione NAT, ritrasmissioni SIP) sono stati risolti.

Tuttavia, il sistema nella sua forma attuale è da considerarsi un prototipo didattico piuttosto che una soluzione production-ready. Per un deployment reale sarebbero necessari: autenticazione robusta, alta disponibilità di RTPEngine, test di carico approfonditi, monitoraggio e alerting, e probabilmente ottimizzazioni per ridurre la latenza di setup (es. ICE trickle).

Nonostante queste limitazioni, il progetto dimostra che l'interoperabilità tra WebRTC moderno e stack SIP legacy (come mjSIP) è tecnicamente realizzabile, anche quando i due protocolli presentano incompatibilità significative a livello di SDP e media transport.
